# ğŸ§  Nano Language Model â€” Apuntes

Modelo que transforma lenguaje natural (espaÃ±ol) en fÃ³rmulas de lÃ³gica proposicional.

**Ejemplo**:
```
Input:  "Si llueve y no llevo paraguas, entonces me mojo"
Output: (p âˆ§ Â¬q) â†’ r
```

---

## Pipeline completo (el camino del dato al modelo)

```
dataset.json (2,500 raw de DeepSeek)
    â†“  clean.py
dataset_clean.json (~2,100)
    â†“  verify.py ($0.50)
dataset_verified.json (~2,000)
    â†“  augment.py
dataset_augmented.json (~5,500+)
    â†“  preprocess.py
train.jsonl / val.jsonl / test.jsonl
    â†“  train tokenizer (BPE)
tokenizer.json
    â†“  train.py
model.pt ğŸ‰
```

---

## Fundamentos de Deep Learning (TLDR)

### Tensores â€” la unidad bÃ¡sica

Un tensor es un contenedor de nÃºmeros con forma (shape). Todo en deep learning son tensores:

```
Escalar (0D):     5                       â†’ shape: ()        â†’ un solo nÃºmero
Vector (1D):      [1, 2, 3]               â†’ shape: (3,)      â†’ una lista
Matriz (2D):      [[1,2], [3,4]]          â†’ shape: (2, 2)    â†’ una tabla
Tensor 3D:        [[[...], [...]], ...]    â†’ shape: (8, 10, 512)  â†’ un cubo
```

Â¿Por quÃ© tensores y no listas de Python? Porque los tensores viven en la GPU
y las operaciones son **miles de veces mÃ¡s rÃ¡pidas**. Una multiplicaciÃ³n de
matrices 512x512 en CPU tarda segundos; en GPU con tensores, microsegundos.

En PyTorch:
```python
import torch
x = torch.tensor([1.0, 2.0, 3.0])    # vector
x = torch.randn(8, 10, 512)           # tensor 3D con valores aleatorios
# 8 = batch_size (cuÃ¡ntos ejemplos procesar a la vez)
# 10 = seq_len (largo de la secuencia)
# 512 = d_model (dimensiÃ³n de representaciÃ³n de cada token)
```

### Neuronas y capas

Una neurona hace: `inputs x pesos -> suma -> activacion -> output`

```
inputs  = [0.5, -1.0, 2.0]
pesos   = [0.3,  0.7, 0.1]   <-- estos se APRENDEN
suma    = 0.5*0.3 + (-1.0)*0.7 + 2.0*0.1 = -0.35
output  = activacion(-0.35)   <-- funcion no lineal (ReLU, SiLU, etc.)
```

Muchas neuronas juntas = una **capa (layer)**.
Muchas capas apiladas = una **red neuronal**.

En PyTorch, una capa lineal es:
```python
capa = nn.Linear(512, 256)   # 512 entradas, 256 salidas
# internamente: output = input @ weight.T + bias
# weight tiene shape (256, 512) = 131,072 parametros aprendibles
```

### Multiplicacion de matrices â€” LA operacion del deep learning

Todo en un Transformer se reduce a multiplicaciones de matrices:

```
(batch, seq, 512) @ (512, 256) = (batch, seq, 256)
     input            weight         output

Regla: el ultimo eje de A debe coincidir con el penultimo de B:
  (8, 10, 512) @ (512, 256) -> (8, 10, 256)  OK
  (8, 10, 512) @ (256, 512) -> ERROR          los ejes no coinciden
```

### El ciclo de aprendizaje (TODO el deep learning son estos 4 pasos)

```
1. FORWARD:    input -> modelo -> prediccion
2. LOSS:       comparar prediccion vs respuesta correcta -> numero de error
3. BACKWARD:   calcular gradientes (cuanto contribuyo cada peso al error?)
4. UPDATE:     ajustar pesos para reducir el error

Repetir miles de veces -> el modelo "aprende"
```

En codigo:
```python
for batch in dataloader:
    pred = model(batch.input)             # 1. forward pass
    loss = loss_fn(pred, batch.target)     # 2. calcular error
    loss.backward()                        # 3. calcular gradientes (automatico!)
    optimizer.step()                       # 4. actualizar pesos
    optimizer.zero_grad()                  # limpiar para la siguiente iteracion
```

### Gradientes â€” como "aprende" el modelo

El gradiente de un peso dice: "si muevo este peso un poquito, el error sube o baja?"

```
peso=0.5, gradiente=-0.1 -> "si subo peso, error BAJA" -> subir
peso=0.5, gradiente=+0.3 -> "si subo peso, error SUBE" -> bajar
```

PyTorch calcula TODOS los gradientes automaticamente con `loss.backward()`.
No necesitas hacer calculo a mano. Esto se llama **autograd** y es la magia
central de PyTorch.

### Loss function â€” como medir el error

Para modelos de lenguaje, se usa **Cross-Entropy Loss**:

```
prediccion del modelo:  [0.1, 0.05, 0.7, 0.15]   (probabilidades para cada token)
respuesta correcta:     [0,   0,    1,   0   ]    (el token correcto es el 3ro)

cross_entropy = -log(0.7) = 0.36    (bajo = bueno, la prediccion era buena)

si hubiera predicho:    [0.6, 0.2, 0.1, 0.1]
cross_entropy = -log(0.1) = 2.30    (alto = malo, fallo la prediccion)
```

El modelo minimiza este numero. Cuando loss baja, el modelo esta aprendiendo.

### nn.Parameter vs nn.Module â€” el sistema de PyTorch

```python
nn.Parameter:  un tensor que PyTorch sabe que debe entrenar
               (calcula gradientes y actualiza con el optimizador)
               Ejemplo: los pesos gamma de RMSNorm

nn.Module:     una "pieza" del modelo. Puede contener Parameters y otros Modules.
               Tiene un metodo forward() que define como transforma el input.
               Ejemplo: RMSNorm, Linear, nuestro Transformer completo

nn.Linear:     Module predefinido. Hace: output = input @ weight + bias
nn.Embedding:  Module predefinido. Es una tabla: ID -> vector de d_model dimensiones
nn.Dropout:    Module predefinido. Apaga neuronas al azar (regularizacion)
```

### Conexion residual â€” el truco que permite redes profundas

Sin residuales, con 8+ capas la seÃ±al se degrada (se pierde informacion):
```
input -> capa1 -> capa2 -> ... -> capa8 -> output  (la seÃ±al original se perdio)
```

Con residuales, sumamos el input original al output de cada capa:
```
input -> capa1 -> (+input) -> capa2 -> (+) -> ... -> output  (seÃ±al preservada)
```

Esto crea un "atajo" para que la informacion fluya directo de las primeras capas
a las ultimas. Sin esto, seria imposible entrenar Transformers profundos.

### La intuicion completa para NanoLogic

```
"Si llueve me mojo"
     | tokenizer
[45, 892, 12, 567]              <-- IDs numericos
     | embedding (tabla de lookup)
[[0.1, -0.3, ...],             <-- cada ID se convierte en un vector de 512 numeros
 [0.5,  0.2, ...],                 que "representan" el significado de la palabra
 [0.8, -0.1, ...],
 [0.2,  0.6, ...]]
     | 8 capas de transformer (attention + FFN)
[[...],                         <-- los vectores se transforman capa a capa
 [...],                             cada capa "entiende" mas contexto
 [...],                             la capa 1 ve palabras individuales
 [...]]                             la capa 8 entiende relaciones logicas
     | linear head
[[0.01, 0.02, ..., 0.95],      <-- probabilidades sobre los 8000 tokens
 [...],                             del vocabulario
 [...],
 [...]]                             "Â¿cual es el siguiente token mas probable?"
     | argmax (tomar el mas probable)
"p" -> "â†’" -> "q"              <-- la formula generada token a token
```

---

## Paso 1: Limpieza (`data/scripts/clean.py`)

**Â¿QuÃ© hace?** Filtra los ejemplos basura que generÃ³ DeepSeek.

**Â¿CÃ³mo?** Usa un **Recursive Descent Parser** â€” un parser que tiene una funciÃ³n
por cada regla de la gramÃ¡tica de lÃ³gica proposicional:

```
fÃ³rmula     â†’ bicondicional
bicondicional â†’ implicaciÃ³n (â†” implicaciÃ³n)*
implicaciÃ³n â†’ disyunciÃ³n (â†’ disyunciÃ³n)*
disyunciÃ³n  â†’ conjunciÃ³n (âˆ¨ conjunciÃ³n)*
conjunciÃ³n  â†’ unario (âˆ§ unario)*
unario      â†’ Â¬ unario | primario
primario    â†’ Ã¡tomo | ( fÃ³rmula )
```

Cada nivel = una precedencia de operador. Los de arriba se evalÃºan Ãºltimo (â†”),
los de abajo primero (Â¬). Como en matemÃ¡ticas: Ã— antes que +.

**Sub-pasos:**
1. Validar sintaxis con el parser (si no parsea â†’ fuera)
2. Verificar parÃ©ntesis balanceados
3. Verificar que los Ã¡tomos declarados coincidan con los de la fÃ³rmula
4. Eliminar duplicados exactos (mismo input + misma fÃ³rmula)
5. Subsampling de fÃ³rmulas triviales (demasiados `p â†’ q` simples)
6. Normalizar formato (espacios, parÃ©ntesis consistentes)

**Resultado**: ~2,100 ejemplos limpios (de 2,500 originales).

---

## Paso 2: VerificaciÃ³n con API (`data/scripts/verify.py`)

**Â¿QuÃ© hace?** Le pregunta a DeepSeek: "Â¿esta fÃ³rmula es correcta para este enunciado?"

**Â¿Por quÃ©?** Porque DeepSeek generÃ³ los datos Y los puede verificar (como peer review).
Un ejemplo puede pasar la validaciÃ³n sintÃ¡ctica pero ser semÃ¡nticamente incorrecto:

```
Input:   "Si llueve O nieva, llevo abrigo"
FÃ³rmula: p âˆ§ q â†’ r    â† SINTÃCTICAMENTE VÃLIDA, pero deberÃ­a ser âˆ¨, no âˆ§
```

El parser no detecta eso. La API sÃ­.

**Costo**: ~$0.50 para 2,000 ejemplos (temperatura baja, respuestas cortas).

**Resultado**: ~2,000 ejemplos verificados.

---

## Paso 3: Data Augmentation (`data/scripts/augment.py`)

**Â¿QuÃ© hace?** Crea ejemplos nuevos a partir de los existentes usando equivalencias lÃ³gicas.

**TÃ©cnicas (Python puro, $0):**

| TÃ©cnica | Ejemplo |
|---------|---------|
| Equivalencia de implicaciÃ³n | `p â†’ q` se convierte en `Â¬p âˆ¨ q` |
| De Morgan (AND) | `Â¬(p âˆ§ q)` se convierte en `Â¬p âˆ¨ Â¬q` |
| De Morgan (OR) | `Â¬(p âˆ¨ q)` se convierte en `Â¬p âˆ§ Â¬q` |
| Doble negaciÃ³n | `Â¬Â¬p` se convierte en `p` (y viceversa) |
| Conmutatividad | `p âˆ§ q` se convierte en `q âˆ§ p` |
| ComposiciÃ³n | Combina 2 ejemplos simples en 1 avanzado |

**Importante**: la frase en lenguaje natural queda igual, pero la fÃ³rmula cambia
a su equivalente. El modelo asÃ­ aprende que hay mÃºltiples representaciones correctas.

**Resultado**: ~5,500+ ejemplos.

---

## Paso 4: Preprocesamiento (`data/scripts/preprocess.py`)

**Â¿QuÃ© hace?** Prepara los datos en el formato exacto que el modelo necesita.

### 4a. Balanceo
- Por complejidad: ~33% simple, 33% intermediate, 33% advanced
- Por bloque/dominio: ninguno domina mÃ¡s del 25%
- Reporte de distribuciÃ³n de conectores y Ã¡tomos

### 4b. Formato con special tokens

Todo ejemplo se convierte en una secuencia con tokens especiales que le dicen
al modelo dÃ³nde empieza cada secciÃ³n:

**Fase 1 (con Chain-of-Thought):**
```
<|input|> Si el server crashea y no hay backup, se pierden los datos
<|output|>
<|thought|> "si...entonces" indica implicaciÃ³n, "y" indica conjunciÃ³n, "no" indica negaciÃ³n
<|atoms|> p: el server crashea | q: hay backup | r: se pierden los datos
<|connectors|> âˆ§: y | Â¬: no | â†’: si...entonces
<|formula|> (p âˆ§ Â¬q) â†’ r
<|end|>
```

**Fase 2 (sin thought â€” para Chain-of-Thought Distillation):**
```
<|input|> Si el server crashea y no hay backup, se pierden los datos
<|output|>
<|atoms|> p: el server crashea | q: hay backup | r: se pierden los datos
<|connectors|> âˆ§: y | Â¬: no | â†’: si...entonces
<|formula|> (p âˆ§ Â¬q) â†’ r
<|end|>
```

### 4c. Split (80/10/10)
- Split por **patrÃ³n lÃ³gico**, no random
- Ejemplo: si `(X âˆ§ X) â†’ X` aparece en train, no aparece en test
- AsÃ­ medimos si el modelo generaliza a estructuras nuevas

### 4d. Curriculum ordering
- Train se ordena: simple â†’ intermediate â†’ advanced
- El modelo aprende lo fÃ¡cil primero (como un humano)

**Resultado**: `train.jsonl`, `val.jsonl`, `test.jsonl`

---

## Paso 5: Entrenar tokenizer (BPE)

**Â¿QuÃ© es BPE?** Byte-Pair Encoding â€” un algoritmo que aprende a dividir texto
en sub-palabras basÃ¡ndose en frecuencia. Ejemplo:

```
"firewall" â†’ ["fire", "wall"]     (palabras comunes se mantienen)
"bypassear" â†’ ["by", "pass", "ear"]  (palabras raras se dividen)
"âˆ§" â†’ ["âˆ§"]                       (sÃ­mbolos lÃ³gicos = 1 token)
```

Se entrena sobre TODO el corpus (frases + fÃ³rmulas). AsÃ­ el vocab cubre
tanto espaÃ±ol como los sÃ­mbolos lÃ³gicos.

---

## Paso 6: Construir el modelo (Transformer Decoder-Only)

### DecisiÃ³n: Decoder-Only (no Encoder-Decoder)

**Encoder-Decoder** = 2 transformers (uno lee, otro genera). Usado por T5, BART.
**Decoder-Only** = 1 transformer que lee y genera en flujo continuo. Usado por GPT, LLaMA, Mistral.

**Â¿Por quÃ© Decoder-Only?**
1. El formato del dataset es secuencial (`<|input|>...<|output|>...<|formula|>...`) â€” es exactamente cÃ³mo funciona Decoder-Only
2. CoT Distillation funciona natural: en Fase 2 solo quitÃ¡s tokens de la secuencia
3. Un solo transformer = menos parÃ¡metros, cabe holgado en T4
4. Todas las tÃ©cnicas avanzadas (Progressive Pruning, Contrastive Examples) fueron diseÃ±adas para Decoder-Only

**Â¿Y no se pierde calidad?** No. Las capas que procesan `<|input|>` funcionan
como un "encoder implÃ­cito". Cuando el modelo llega a `<|output|>`, sus hidden
states ya contienen toda la comprensiÃ³n. No necesitÃ¡s encoder separado.

### Especificaciones (~15M parÃ¡metros, T4)

```
TODO: definir juntos cuando lleguemos aquÃ­
- d_model: ?
- n_heads: ?
- n_layers: ?
- d_ff: ?
- max_seq_len: ?
- vocab_size: ? (lo determina el BPE)
```

### Componentes state of the art:

#### âœ… Causal Masking â€” "No ver el futuro"

Durante el entrenamiento, el modelo procesa TODA la secuencia de una vez (en paralelo).
Pero cada token solo deberÃ­a poder ver los tokens **anteriores**, no los que vienen despuÃ©s.

La causal mask es una matriz triangular que bloquea el futuro:

```
Tokens:    A    B    C    D
A          âœ…   âŒ   âŒ   âŒ     â† A solo se ve a sÃ­ mismo
B          âœ…   âœ…   âŒ   âŒ     â† B ve A y a sÃ­ mismo
C          âœ…   âœ…   âœ…   âŒ     â† C ve A, B y a sÃ­ mismo
D          âœ…   âœ…   âœ…   âœ…     â† D ve todo
```

Se implementa poniendo **-infinito** en las posiciones futuras antes del softmax:

```python
mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
mask = mask.masked_fill(mask == 1, float('-inf'))
# Softmax convierte -inf â†’ 0.00 (atenciÃ³n cero al futuro)
```

**Â¿Por quÃ© importa?** Sin esto, el modelo harÃ­a trampa durante entrenamiento
(verÃ­a las respuestas). Con la mask, cada posiciÃ³n es un "mini-examen" independiente.
Esto permite entrenar en paralelo ("teacher forcing") pero simular generaciÃ³n secuencial.

Durante **inferencia** (cuando generÃ¡s), la mask no hace falta porque literalmente
no hay futuro â€” el modelo solo ve lo que ya generÃ³.

---

#### âœ… RoPE (Rotary Position Embeddings) â€” "Â¿DÃ³nde estoy?"

**Problema**: El Transformer es ciego a posiciones. "gato come pez" y "pez come gato"
le parecen iguales â€” mismos tokens, diferente orden.

**SoluciÃ³n vieja (2017)**: sumar un vector de posiciÃ³n al embedding. Funciona, pero
codifica posiciones **absolutas** â€” "estoy en la posiciÃ³n 5". Lo que importa en lenguaje
es la distancia **relativa** â€” "estoy 3 tokens despuÃ©s de esa palabra".

**RoPE (2021)**: en vez de sumar, **rota** el vector del embedding:

```
PosiciÃ³n 0:  â†’      (sin rotar)
PosiciÃ³n 1:  â†—      (rotada 10Â°)
PosiciÃ³n 2:  â†‘      (rotada 20Â°)
PosiciÃ³n 3:  â†–      (rotada 30Â°)
```

**El truco matemÃ¡tico**: cuando dos tokens calculan atenciÃ³n (dot product de sus
vectores rotados), el resultado solo depende de la **diferencia de posiciones**:

```
dot(pos_5, pos_8) depende de 8-5 = 3
dot(pos_2, pos_5) depende de 5-2 = 3  â† Â¡mismo resultado!
```

**En dimensiones altas** (d_model=256): Se agrupan en pares y cada par rota con
frecuencia diferente â€” como tener un segundero (detalle fino), minutero (medio)
y horario (largo plazo) para cubrir todas las escalas de distancia.

**Se aplica solo a Q y K** (query/key), NO a V (value):
- Q Ã— K (con RoPE) = "**a quiÃ©n** presto atenciÃ³n" (necesita posiciones)
- V (sin RoPE) = "**quÃ© informaciÃ³n** recojo" (no necesita posiciones)

**Lo usan**: LLaMA, Mistral, Qwen, Gemma, y casi todos los modelos modernos.

---

#### âœ… ALiBi (Attention with Linear Biases) â€” alternativa a RoPE

En vez de modificar embeddings, **penaliza directamente los scores de atenciÃ³n**
segÃºn la distancia entre tokens:

```
Score final = QÂ·K âˆ’ m Ã— distancia

Head 1: pendiente m = 1/2   (penaliza suave â†’ ve "lejos")
Head 2: pendiente m = 1/4
Head 3: pendiente m = 1/8
Head 4: pendiente m = 1/16  (penaliza fuerte â†’ se enfoca en lo "cercano")
```

Ventaja: **extrapola muy bien** a secuencias mÃ¡s largas que las de entrenamiento.
Usado por BLOOM, MPT, Falcon.

**Â¿Por quÃ© NO lo usamos?** Nuestras secuencias son cortas (~100-200 tokens),
asÃ­ que la extrapolaciÃ³n no importa. RoPE captura relaciones posicionales mÃ¡s ricas
que una simple resta lineal, y todo el ecosistema moderno (LLaMA, Mistral) usa RoPE.

---

#### âœ… SwiGLU â€” activaciÃ³n con compuerta inteligente

**EvoluciÃ³n de las activaciones:**

```
ReLU (2012):  max(0, x)              â€” simple pero neuronas "mueren" con valores negativos
GELU (2016):  x Â· Î¦(x)              â€” mÃ¡s suave, no mueren, pero no filtra
SwiGLU (2020): Swish(gate) Ã— content â€” FILTRA selectivamente
```

**FFN clÃ¡sico** (con ReLU/GELU): una sola rama
```
input â†’ Wâ‚ â†’ activaciÃ³n â†’ Wâ‚‚ â†’ output
```

**FFN con SwiGLU**: dos ramas, una actÃºa como COMPUERTA de la otra
```
         input
        /     \
    Wâ‚(x)   W_gate(x)
      |         |
      |      Swish()
      |         |
      Ã—â”€â”€â”€â”€â”€â”€â”€â”€â”€Ã—     â† multiplicaciÃ³n: la gate decide quÃ© pasa
      |
    Wâ‚‚(x)
      |
    output
```

**Â¿Por quÃ© es mejor?** La compuerta aprende a **bloquear selectivamente**
informaciÃ³n irrelevante para cada token. ReLU/GELU aplican la misma transformaciÃ³n
a todas las dimensiones â€” SwiGLU puede decir "para este token, deja pasar
las dimensiones de polaridad y bloquea las de sustantivo".

**Costo**: 3 matrices en vez de 2. Se compensa reduciendo d_ff:
```
FFN clÃ¡sico:  d_ff = 4 Ã— d_model
FFN SwiGLU:   d_ff = (8/3) Ã— d_model â‰ˆ 2.67 Ã— d_model
```
Mismos parÃ¡metros, mejor rendimiento.

**Lo usan**: LLaMA 1/2/3, Mistral, Gemma, PaLM â€” todos desde 2023.

---

#### âœ… RMSNorm â€” normalizaciÃ³n mÃ¡s eficiente que LayerNorm

LayerNorm hace: `(x - mean) / std * gamma + beta` (4 operaciones).
RMSNorm hace: `x / rms(x) * gamma` (2 operaciones).
El paper demostrÃ³ que restar la media aporta casi nada. Resultado: misma calidad, ~10-15% mÃ¡s rÃ¡pido.
Lo usan: LLaMA 1/2/3, Mistral, Gemma.

Se usa 17 veces en nuestro modelo:
- 2 por capa (antes de attention + antes de FFN) x 8 capas = 16
- 1 final antes del head = 1
- Total params: 17 x 512 = 8,704 (0.03% del modelo, baratÃ­simo pero crÃ­tico)

Detalles de implementaciÃ³n:
- `rsqrt` en vez de `sqrt` + divisiÃ³n (mÃ¡s rÃ¡pido)
- Upcast a float32 para la normalizaciÃ³n (evita overflow en float16/bfloat16)
- Solo parÃ¡metro `gamma` (sin bias), empieza en 1.0

**Deep Norm** (Microsoft Research â€” "DeepNet: Scaling Transformers to 1,000 Layers"):
Escalar la conexiÃ³n residual por alpha para que la seÃ±al original llegue fuerte
a las capas profundas: `output = x * alpha + sublayer(norm(x))`
- alpha = (2 * n_layers)^0.25 = 2.0 con 8 capas
- beta = (8 * n_layers)^-0.25 = 0.354 para inicializaciÃ³n de pesos

---

#### âœ… Attention Deep Dive â€” como funciona de verdad

##### Â¿QuÃ© es un tensor realmente?

Un tensor es una caja de nÃºmeros con dimensiones (shape).
Una **matriz** es un tensor de 2 dimensiones. Un tensor puede tener cualquier cantidad.

En nuestro modelo, el tensor principal tiene esta forma:
```
(8, 100, 512)
 â”‚   â”‚    â””â”€â”€ 512 nÃºmeros que "describen" cada token
 â”‚   â””â”€â”€ 100 tokens en la secuencia
 â””â”€â”€ 8 secuencias procesadas al mismo tiempo (batch)
```

Â¿Que significan esos 512 numeros por token? Cada uno captura una "propiedad" aprendida:
```
"llueve" = [0.8, -0.3, 0.1, 0.9, ..., -0.2]
            â”‚     â”‚     â”‚    â”‚
            â”‚     â”‚     â”‚    â””â”€â”€ Â¿es una accion? (0.9 = muy si)
            â”‚     â”‚     â””â”€â”€ Â¿es positivo? (0.1 = neutro)
            â”‚     â””â”€â”€ Â¿es un sustantivo? (-0.3 = no)
            â””â”€â”€ Â¿tiene que ver con clima? (0.8 = mucho)
```

En realidad las dimensiones no tienen nombres claros â€” el modelo aprende
que poner en cada una. Pero palabras similares terminan con vectores similares:
```
"llueve"   = [0.8, -0.3, 0.1, ...]  â”€â” similares
"llovizna" = [0.7, -0.2, 0.1, ...]  â”€â”˜ (clima)
"mesa"     = [-0.5, 0.8, -0.3, ...]    completamente diferente
```

##### Â¿Por que importa la posicion?

Sin posicion, el modelo ve las palabras como bolsa desordenada:
```
"El perro muerde al gato" = {perro, gato, muerde, el, al}
"El gato muerde al perro" = {perro, gato, muerde, el, al}  <-- MISMA BOLSA!
```

El modelo no sabria quien muerde a quien. RoPE rota cada vector segun su posicion:
- PosiciÃ³n 0: "El" â†’ vector sin rotar
- PosiciÃ³n 1: "perro" â†’ vector rotado 1 paso
- PosiciÃ³n 2: "muerde" â†’ vector rotado 2 pasos

La rotacion hace que la similitud Q*K dependa de la **diferencia** de posiciones
(relativa), no de las posiciones absolutas.

##### El problema que Attention resuelve

El modelo procesa: "Si llueve entonces me ____"

Para predecir "mojo", necesita saber que:
- "llueve" es la CAUSA (super relevante)
- "entonces" indica CONSECUENCIA (relevante)
- "me" indica A QUIÃ‰N (algo relevante)
- "Si" indica CONDICIONAL (relevante para la estructura)

Â¿Como decide que es relevante? Con **Q, K, V**.

##### Q, K, V â€” la analogia de la biblioteca

```
Q (Query)  = Tu pregunta: "Necesito info sobre la CAUSA"
K (Key)    = La etiqueta de cada libro: "Yo hablo sobre clima"
V (Value)  = El contenido del libro: la informacion real

Q y K se comparan para decidir que libros abrir.
V es lo que lees de los libros que decidiste abrir.
```

Cada token genera SU PROPIO Q, K y V multiplicando su vector por 3 matrices
de peso (W_Q, W_K, W_V). Estas matrices son los parametros que el modelo aprende.

##### Ejemplo paso a paso con numeros

4 tokens con vectores de 3 dimensiones (simplificado):
```
Tokens: ["Si", "llueve", "me", "mojo"]

Paso 1: Cada token genera Q, K, V
  "Si"     â†’ Q=[1,0,0]  K=[0,1,0]  V=[0.1, 0.2, 0.3]
  "llueve" â†’ Q=[0,1,1]  K=[1,1,0]  V=[0.8, 0.1, 0.5]
  "me"     â†’ Q=[0,0,1]  K=[0,0,1]  V=[0.2, 0.3, 0.1]
  "mojo"   â†’ Q=[1,1,0]  K=[1,0,1]  V=[0.4, 0.6, 0.2]
```

Para "mojo": Â¿a quien deberia prestar atencion?
```
Paso 2: Producto punto de Q_mojo con K de cada token anterior

  Q_mojo = [1, 1, 0]

  Q_mojo Â· K_si     = 1*0 + 1*1 + 0*0 = 1    (algo relevante)
  Q_mojo Â· K_llueve = 1*1 + 1*1 + 0*0 = 2    (MUY relevante!)
  Q_mojo Â· K_me     = 1*0 + 1*0 + 0*1 = 0    (irrelevante)
  Q_mojo Â· K_mojo   = 1*1 + 1*0 + 0*1 = 1    (algo relevante)

  Scores = [1, 2, 0, 1]
```

El **producto punto mide similitud**. Q de "mojo" y K de "llueve"
apuntan en la misma direccion â†’ score alto â†’ atiende a "llueve".
```
Paso 3: Softmax â€” convertir scores a probabilidades

  softmax([1, 2, 0, 1]) = [0.18, 0.49, 0.07, 0.26]
                            â”‚     â”‚      â”‚      â”‚
                            â”‚     49%!   â”‚      â””â”€â”€ "mojo" 26%
                            â”‚  "llueve"  â””â”€â”€ "me" 7%
                            â””â”€â”€ "Si" 18%
```

"llueve" gana con 49% de la atencion.
```
Paso 4: Promediar los V ponderados por las probabilidades

  output = 0.18*V_si + 0.49*V_llueve + 0.07*V_me + 0.26*V_mojo
         = [0.52, 0.27, 0.37]
```

Este vector de salida ahora "sabe" que "mojo" esta relacionado con "llueve".
El vector original de "mojo" no tenia esa informacion. Despues de attention, si.

##### Â¿Por que Multi-Head?

Una sola cabeza solo captura UNA relacion. Las palabras tienen muchas:
```
Cabeza 1: relaciones de CAUSA    â†’ "mojo" atiende a "llueve"
Cabeza 2: relaciones de SINTAXIS â†’ "mojo" atiende a "me" (sujeto)
Cabeza 3: relaciones de POSICION â†’ "mojo" atiende a "entonces"
... (8 cabezas en nuestro modelo)
```

Cada cabeza tiene sus propias matrices W_Q, W_K, W_V â†’ aprende a buscar
un tipo diferente de relacion. Despues se concatenan los resultados.

##### Causal Mask â€” prohibir ver el futuro

En entrenamiento procesamos TODA la oracion a la vez, pero cada token solo
puede ver los anteriores (sino haria trampa):
```
         Si  llueve  me  mojo
Si     [ OK   -inf  -inf  -inf ]   â† solo se ve a si mismo
llueve [ OK    OK   -inf  -inf ]   â† ve Si y a si mismo
me     [ OK    OK    OK   -inf ]   â† ve los 3 anteriores
mojo   [ OK    OK    OK    OK  ]   â† ve todo
```

Los -inf se eliminan con softmax(-inf) = 0.

##### Differential Attention â€” cancelar ruido (Microsoft 2024)

El softmax normal asigna atencion no-cero a TODOS los tokens, incluso
los irrelevantes. Differential Attention calcula DOS patrones y los resta:
```
attn = softmax(Q1*K1) - lambda * softmax(Q2*K2)

El ruido (similar en ambos patrones) se cancela.
Solo queda la seÃ±al real: los tokens que realmente importan.
```

Lambda es un parametro aprendible â€” el modelo decide cuanto ruido cancelar.

---

#### âœ… FFN Deep Dive â€” la "memoria interna" del Transformer

##### Â¿Que hace la FFN?

Si attention es "a quien presto atencion", la FFN es
"ahora que ya se que es relevante, que hago con esa informacion?"

```
Attention: RECOPILAR informacion de otros tokens
FFN:       PROCESAR esa informacion â†’ "pensar"
```

Estudios (Geva et al., 2021) demostraron que las FFN funcionan como
MEMORIA ASOCIATIVA: cada neurona almacena un patron (key) y una respuesta (value).
Es literalmente otra forma de attention pero sobre "memorias" aprendidas.

##### Â¿Por que SwiGLU y no FFN clasica?

```
FFN clasica (GPT-2):
  x = Linear(512 -> 2048)   # expandir 4x
  x = ReLU(x)               # filtro binario: pasa o no pasa
  x = Linear(2048 -> 512)   # comprimir

SwiGLU (LLaMA/Mistral/Gemma):
  gate = Linear(512 -> 1365)    # la compuerta
  up   = Linear(512 -> 1365)    # el contenido  
  x    = SiLU(gate) * up        # compuerta FILTRA SELECTIVAMENTE
  x    = Linear(1365 -> 512)    # comprimir
```

ReLU es binario (si/no). SwiGLU tiene una compuerta suave que aprende
QUE dimensiones bloquear para cada token. Puede decir "para llueve,
deja pasar las dimensiones de clima y bloquea las de sintaxis".

Costo: 3 matrices en vez de 2 â†’ se compensa con d_ff mas chico:
- FFN clasica: d_ff = 4 x d_model = 2048
- SwiGLU: d_ff = (8/3) x d_model = 1365
- Mismos parametros, mejor rendimiento.

##### Gate Residual (truco underground)

Agregar un bypass aprendible dentro de la compuerta:
```
Normal:          output = SiLU(gate) * up
Gate Residual:   output = SiLU(gate) * up + alpha * up
```

Si la compuerta se equivoca y bloquea algo importante,
`alpha * up` permite que pase igualmente.
Alpha empieza en 0 (sin efecto) y el modelo aprende si necesita usarlo.
Costo: 1 parametro escalar extra. Riesgo: cero.

##### Parametros de la FFN

```
gate_proj:  512 x 1365 =  698,880
up_proj:    512 x 1365 =  698,880
down_proj:  1365 x 512 =  698,880
Total:                   2,096,640 por capa
x 8 capas:             16,773,120 (~64% del modelo)
```

La FFN es por lejos el componente MAS caro en parametros.

---

#### ğŸ’¡ MoE (Mixture of Experts) â€” por que NO lo usamos (aun)

MoE = N FFNs pequeÃ±as especializadas + un router que elige cuales activar:
```
Token "llueve" â†’ Router â†’ Experto 2 (clima) + Experto 4 (semantica)
                          Los otros no se activan â†’ ahorra computo
```

Â¿Por que no ahora?
- **Data insuficiente**: con 6,080 ejemplos, cada experto veria ~3,000.
  No hay suficiente diversidad para que se especialicen de verdad.
- **Riesgo de colapso**: los expertos terminan haciendo todos lo mismo.
- **Complejidad**: router, load balancing, auxiliary loss.

Â¿Cuando si? Con 20K+ ejemplos Y multiples tareas (NLâ†’logica, logicaâ†’NL,
verificacion, simplificacion). Ahi cada experto podria especializarse.

La FFN esta diseÃ±ada modular para poder swapear a MoE en el futuro.

---

#### âœ… TransformerBlock â€” la pieza LEGO que se repite

El bloque es la unidad que se repite 8 veces. Cada bloque ensambla todo:
```
input
  |
  |---- [RMSNorm] -> [Attention] -----|
  |                                    |
  |--- (* alpha) -------------------- (+) -- residual 1
                                       |
  |---- [RMSNorm] -> [SwiGLU FFN] ----|
  |                                    |
  |--- (* alpha) -------------------- (+) -- residual 2
                                       |
                                    output
```

Cada capa aprende relaciones diferentes:
- Capas tempranas (1-3): relaciones simples (sintaxis, palabras cercanas)
- Capas medias (4-6): relaciones semanticas (significado, roles)
- Capas tardias (7-8): relaciones abstractas (logica, implicaciones)

##### Pre-Norm vs Post-Norm

```
Post-Norm (GPT-2):   output = Norm(x + sublayer(x))
Pre-Norm (LLaMA):    output = x + sublayer(Norm(x))     <-- usamos esta
```

Pre-Norm deja la conexion residual como camino limpio sin obstaculos.
Post-Norm pone la normalizacion EN el camino del residual, lo que puede
amortiguar seÃ±ales importantes en redes profundas.

##### Stochastic Depth â€” regularizacion a nivel de bloque

Durante entrenamiento, saltarse bloques al azar con probabilidad creciente:
```
Capa 0: p=0.000 (nunca se salta â€” es critica)
Capa 3: p=0.043
Capa 7: p=0.100 (10% de chance de skip)
```

Â¿Por que funciona?
- Obliga al modelo a no depender de una sola capa
- Las capas profundas son mas "redundantes" â†’ se saltean mas
- En inferencia: todos los bloques se ejecutan (escalados por 1/(1-p))
- Zero parametros extra, zero overhead en inferencia
- Con solo 6K datos, toda regularizacion extra ayuda

##### Â¿Por que NO Parallel Attention + FFN?

PaLM/GPT-J ejecutan attention y FFN en paralelo:
```
Paralelo:    output = x + attention(norm(x)) + ffn(norm(x))
Secuencial:  output = x + ffn(norm(x + attention(norm(x))))    <-- usamos esta
```

En modelos grandes no pierde calidad. En modelos chicos (< 1B params),
la secuencialidad importa: la FFN necesita ver el OUTPUT de attention
(ya enriquecido con contexto), no el input crudo.

##### Â¿Por que NO Adaptive Depth?

Adaptive Depth deja que el modelo decida cuantas capas usar por token.
No lo usamos porque:
- Necesita un loss auxiliar ("ponder cost") dificil de calibrar
- Con 6K datos, el halting score no aprende bien cuando parar
- Termina usando siempre todas las capas â†’ mismo resultado, mas overhead
- Con 100K+ datos seria viable

---

#### âœ… NanoLogicTransformer â€” el modelo completo

El transformer ensambla todo en un flujo end-to-end:
```
Token IDs [45, 892, 12, 567]
     |
  Embedding (8000x512) â†’ vectores    (* sqrt(512) para escalar)
     |
  8 x TransformerBlock               (attention + FFN + Deep Norm)
     |
  RMSNorm final                      (estabilizar antes de salida)
     |
  LM Head (512â†’8000)                 (pesos compartidos con Embedding!)
     |  * head_scale                  (calibracion aprendible)
     |  tanh soft-capping             (anti-overconfidence)
     |
  logits â†’ loss / token predicho
```

##### Weight Tying â€” compartir pesos

Embedding (8000x512) convierte token_id â†’ vector (buscar significado).
LM Head (512x8000) convierte vector â†’ token_id (predecir token).
Son operaciones INVERSAS â†’ compartir la misma matriz:
- Ahorra 4.1M params (16% del modelo)
- Fuerza consistencia semantica
- Actua como regularizacion
Lo usan: GPT-2, LLaMA, Mistral, Gemma â€” TODOS.

##### Output Soft-Capping (Gemma 2)

Igual que attention soft-capping pero para los logits finales:
`logits = 30 * tanh(logits / 30)`
Previene overconfidence: el modelo no puede estar 100% seguro.

##### Head Scale â€” calibracion aprendible

`logits = lm_head(x) * head_scale` â€” head_scale empieza en 1.0.
El modelo puede aprender a ser mas cauteloso (< 1) o mas seguro (> 1).
Un solo parametro, zero riesgo.

##### Z-Loss (PaLM) â€” regularizacion suave

`z_loss = 1e-4 * mean(logsumexp(logits)^2)`
EnseÃ±a al modelo a mantener logits en rango razonable.
Complementario a soft-capping: cap limita "a la fuerza",
z-loss enseÃ±a a no necesitar limitacion.

##### Deep Norm beta init

Los sublayers se inicializan con pesos escalados por:
`beta = (8 * n_layers)^-0.25 = 0.354`
Al inicio los sublayers contribuyen POCO y los residuales dominan.
El modelo gradualmente aprende a "abrir" los sublayers.

##### Â¿Por que NO uP (Maximal Update Parameterization)?

uP permite transferir hiperparametros de modelos chicos a grandes.
Pero requiere scaling experiments (entrenar multiples tamaÃ±os).
Nosotros tenemos UN solo modelo de UN solo tamaÃ±o â€” no hay nada
que "transferir". Lo que si robamos de uP es la idea de escalar
la inicializacion por el ancho, que ya hacemos con Deep Norm beta.

---

#### âœ… Special Tokens â€” el protocolo de comunicaciÃ³n del modelo

Son tokens inventados que NO existen en el lenguaje natural. Le dan estructura
a las secuencias para que el modelo sepa quÃ© es quÃ©.

Formato de una secuencia de entrenamiento:
```
<|bos|><|input|> Si llueve me mojo <|output|><|thought|> "si...entonces"
indica implicaciÃ³n <|atoms|> p: llueve | q: me mojo <|connectors|>
â†’: si...entonces <|formula|> p â†’ q <|eos|>
```

Tokens definidos:

| Token | Nombre | FunciÃ³n |
|-------|--------|--------|
| `<\|pad\|>` | Padding | Relleno para igualar largo en batches |
| `<\|bos\|>` | Begin of Seq | "AquÃ­ empieza todo" |
| `<\|eos\|>` | End of Seq | "Ya terminÃ©" â€” el modelo para de generar |
| `<\|unk\|>` | Unknown | Caracteres desconocidos (no deberÃ­a aparecer) |
| `<\|input\|>` | Input | AquÃ­ va el texto en espaÃ±ol |
| `<\|output\|>` | Output | AquÃ­ empieza lo que el modelo genera |
| `<\|thought\|>` | Thought | Razonamiento paso a paso (CoT, Fase 1) |
| `<\|atoms\|>` | Atoms | Ãtomos proposicionales identificados |
| `<\|connectors\|>` | Connectors | Conectores lÃ³gicos identificados |
| `<\|formula\|>` | Formula | La fÃ³rmula final (salida principal) |

**Â¿Por quÃ© `<|...|>`?** EstÃ¡ndar de la industria (GPT, LLaMA, Mistral).
Los delimitadores hacen imposible confundir un token especial con texto normal.

**Â¿Por quÃ© `<|eos|>` y no `<|end|>`?** EOS (End of Sequence) es el nombre estÃ¡ndar
en todos los modelos modernos. Consistencia con el ecosistema.

Archivo: `src/tokenizer/special_tokens.py` â€” dataclass frozen (inmutable).

---

#### âœ… BPE (Byte Pair Encoding) â€” cÃ³mo el modelo "lee" texto

El modelo trabaja con nÃºmeros, no texto. BPE convierte texto â†’ tokens (nÃºmeros).

**3 enfoques posibles:**
- Por caracteres: `"llueve"` â†’ `["l","l","u","e","v","e"]` â€” secuencias muy largas
- Por palabras: `"llueve"` â†’ `["llueve"]` â€” no maneja palabras nuevas (`<UNK>`)
- **BPE**: punto medio â€” palabras frecuentes enteras, raras en sub-pedazos

**Algoritmo (simplificado):**
```
1. Empezar con caracteres individuales como vocabulario
2. Contar quÃ© PARES de tokens aparecen mÃ¡s seguido
3. Fusionar el par mÃ¡s frecuente en un nuevo token
4. Repetir hasta alcanzar el vocab_size deseado
```

**Ejemplo:**
```
Corpus: "llueve llueve lluvia"

Paso 0: l l u e v e  l l u e v e  l l u v i a
Paso 1: [ll] aparece 3 veces â†’ fusionar â†’ [ll]u e v e ...
Paso 2: [llu] aparece 3 veces â†’ fusionar â†’ [llu] e v e ...
Paso 3: [ev] aparece 2 veces â†’ fusionar â†’ [llu][ev] e ...
...y asÃ­ hasta tener el vocabulario deseado.
```

**Â¿Por quÃ© entrenamos NUESTRO propio BPE?**
Nuestro modelo trabaja con dos "idiomas": espaÃ±ol + lÃ³gica proposicional (â†’, âˆ§, Â¬).
Un tokenizer genÃ©rico no sabrÃ­a manejar los sÃ­mbolos lÃ³gicos. El nuestro sÃ­,
porque lo entrenamos con nuestros datos procesados (6,080 ejemplos).

**ProtecciÃ³n de special tokens:**
```
SIN protecciÃ³n: "<|formula|>" â†’ ["<", "|", "formula", "|", ">"]  â† MAL
CON protecciÃ³n: "<|formula|>" â†’ ["<|formula|>"]                   â† BIEN
```

**Vocab size para NanoLogic: ~4,000-8,000 tokens**
(GPT-2 usa 50K, LLaMA 32K â€” pero ellos cubren TODO el idioma inglÃ©s.
Nosotros solo cubrimos espaÃ±ol + lÃ³gica, asÃ­ que con muchos menos alcanza.)

**Resultado del entrenamiento BPE:**

| Propiedad | Valor |
|-----------|-------|
| Vocab size total | 8,000 |
| Special tokens | 10 (IDs 0-9) |
| BPE tokens | 7,990 |
| PAD ID | 0 |
| BOS ID | 1 |
| EOS ID | 2 |

Todos los special tokens se mantienen como 1 solo token (no se parten) âœ…
Guardado en `models/tokenizer/tokenizer.json`.

Archivos:
- `src/tokenizer/tokenizer.py` â€” wrapper NanoLogicTokenizer (encode, decode, save, load)
- `data/scripts/train_tokenizer.py` â€” script de entrenamiento

**Â¿Se puede mejorar el tokenizer?** SÃ­, pero el impacto es marginal (~1-2%).
Alternativas: WordPiece, Unigram, SentencePiece â€” pero BPE ByteLevel es el estÃ¡ndar
y funciona bien para nuestro caso (espaÃ±ol + lÃ³gica proposicional).

**LecciÃ³n pragmÃ¡tica:** El tokenizer importa menos que la arquitectura y los datos.
Papers de DeepMind y Meta muestran que con un tokenizer "decente" y buenos datos,
el modelo aprende bien. Prioridad de impacto en rendimiento:
1. MÃ¡s datos de calidad (+10-20%)
2. Mejor arquitectura (+5-15%)
3. Mejor training strategy (+5-10%)
4. Mejor tokenizer (+1-2%) â† no vale la pena optimizar primero

**DecisiÃ³n:** dejamos el tokenizer como estÃ¡ y avanzamos al modelo.
Si despuÃ©s vemos que las fÃ³rmulas se tokenizan mal, iteramos.

---

## Paso 6: Arquitectura del modelo

#### âœ… Decoder-Only â€” por quÃ© este tipo de Transformer

3 tipos de Transformers:
- **Encoder-Only** (BERT): clasificaciÃ³n, no genera texto â†’ âŒ
- **Encoder-Decoder** (T5): traducciÃ³n, pero overhead innecesario â†’ ğŸŸ¡
- **Decoder-Only** (GPT, LLaMA, Mistral): generaciÃ³n secuencial â†’ âœ…

Decoder-Only es ideal porque nuestro problema es generaciÃ³n condicional:
dado texto en espaÃ±ol, GENERA la fÃ³rmula token por token.

#### âœ… Config "grado militar" â€” mÃ¡ximo rendimiento por parÃ¡metro

FilosofÃ­a: cada tÃ©cnica que cuesta 0 en computaciÃ³n pero da +1% de rendimiento, la incluimos.

| ParÃ¡metro | Valor | JustificaciÃ³n |
|-----------|-------|---------------|
| `d_model` | 512 | Balance capacidad vs costo |
| `n_layers` | 8 | MÃ¡s profundo = mejor razonamiento |
| `n_heads` | 8 | 512/8=64 dim por head (estÃ¡ndar) |
| `n_kv_heads` | 2 | GQA: ahorra 50% en KV cache |
| `d_ff` | 1365 | SwiGLU: (8/3)*512 |
| `vocab_size` | 8000 | Ya entrenado |
| `max_seq_len` | 1024 | Holgura para secuencias largas |
| `dropout` | 0.1 | RegularizaciÃ³n estÃ¡ndar |

~20M params total. Inspirado en LLaMA 3 + Gemma 2.

#### âœ… Tricks incluidos (estado del arte)

| TÃ©cnica | Â¿QuÃ© hace? | Usado por |
|---------|-----------|-----------|
| **Pre-Norm** | Normalizar ANTES de attention/FFN (mÃ¡s estable) | LLaMA, Mistral |
| **GQA** | Varias heads comparten K,V (50% menos KV) | Mistral, LLaMA 2+ |
| **QK-Norm** | Normalizar Q,K antes de attention (evita explosiÃ³n) | Gemma |
| **Embed Scale** | Escalar embeddings por âˆšd_model | Transformer original |
| **Logit Soft-Capping** | Limitar logits de atenciÃ³n con tanh (estabilidad) | Gemma 2 |
| **Weight Tying** | Embedding y head comparten pesos (ahorra 4M params) | GPT-2, LLaMA |
| **RoPE** | Posiciones relativas por rotaciÃ³n | LLaMA, Mistral |
| **SwiGLU** | FFN con gating (mejor que ReLU/GELU) | LLaMA, Mistral |
| **RMSNorm** | NormalizaciÃ³n sin media (mÃ¡s eficiente) | LLaMA |

#### âœ… Scaling Laws â€” por quÃ© 20M y no 100M

Google Colab T4 tiene 16GB VRAM â€” aguanta modelos de hasta ~150M params.
Pero mÃ¡s params â‰  mejor. Ley de Chinchilla:

> Para que un modelo aproveche sus parÃ¡metros, necesita datos proporcionales.

| Params | Datos necesarios | Â¿Tenemos? |
|--------|-----------------|-----------|
| 20M | ~400K tokens | âœ… SÃ­ |
| 50M | ~1M tokens | ğŸŸ¡ Borderline |
| 100M | ~2M tokens | âŒ No |

Con ~6,080 ejemplos (secuencias largas con CoT), un modelo de 20-25M es el sweet spot.
Si despuÃ©s metemos mÃ¡s datos (Detective, leyes de Chile), escalamos a 30-40M.

#### Estructura de archivos del modelo

```
src/model/
  â”œâ”€â”€ config.py        â† hiperparÃ¡metros (dataclass)
  â”œâ”€â”€ attention.py     â† Multi-Head Attention + RoPE + GQA + QK-Norm
  â”œâ”€â”€ ffn.py           â† SwiGLU Feed-Forward Network
  â”œâ”€â”€ rmsnorm.py       â† RMSNorm
  â”œâ”€â”€ block.py         â† Decoder Block (attention + ffn + residuals)
  â””â”€â”€ transformer.py   â† Modelo completo
```

#### âœ… config.py â€” centralizaciÃ³n de hiperparÃ¡metros

Se usa un `dataclass(frozen=True)` para que nadie modifique los hiperparÃ¡metros
despuÃ©s de crear el config. Si quieres otro config, creas nueva instancia.

Conteo de parÃ¡metros con defaults (~26M brutos, ~22M netos con weight tying):
- Embedding: 8,000 Ã— 512 = 4.1M
- Por capa: Attention (~390K) + SwiGLU FFN (~2.1M) + RMSNorm (~1K) â‰ˆ 2.5M
- 8 capas Ã— 2.5M = 20M
- Head: 0 (weight tying con embedding)

Incluye `count_params_estimate()` para inspeccionar la distribuciÃ³n de parÃ¡metros.
Incluye `to_json()` / `from_json()` para guardar junto al modelo (reproducibilidad).
Validaciones en `__post_init__` atrapan errores inmediatamente (ej: d_model no divisible por n_heads).

---

## Paso 7: Entrenamiento (PyTorch Lightning)

TODO: notas sobre el training loop cuando lleguemos ahÃ­.

Temas pendientes:
- Chain-of-Thought Distillation (2 fases)
- Progressive Output Pruning
- Label Smoothing Selectivo
- Contrastive Examples
- Auxiliary Losses

---

## ğŸ”® VisiÃ³n futura: NanoLogic Stack (3 modelos)

Una vez el Formalizador estÃ© entrenado, el plan es construir un **stack de 3 capas**:

```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Frase â”€â”€â–ºâ”‚  Modelo 1: FORMALIZADOR â”‚â”€â”€â–º fÃ³rmula + Ã¡tomos + conectores
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Modelo 2: VERIFICADOR  â”‚â”€â”€â–º VÃLIDO / INVÃLIDO / FALACIA FORMAL
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   (truth tables, Python puro, NO es modelo)
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Modelo 3: DETECTIVE    â”‚â”€â”€â–º FALACIA INFORMAL detectada
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   + nombre + explicaciÃ³n
```

### Modelo 1: Formalizador (lo que estamos construyendo)
- NL (espaÃ±ol) â†’ fÃ³rmula de lÃ³gica proposicional
- Decoder-Only, ~15M params
- Dataset: ~2,334 ejemplos minados con DeepSeek

### Modelo 2: Verificador (cÃ³digo puro, NO modelo)
- Truth tables + reglas deterministas
- Cero error, verificable, sin incertidumbre
- Detecta falacias FORMALES (ej: afirmaciÃ³n del consecuente)

### Modelo 3: Detective de Falacias Informales (futuro, ~$1.50)
- Detecta lo que la lÃ³gica formal NO atrapa:
  - Hombre de paja, ad hominem, falsa causa, pendiente resbaladiza
- Dataset nuevo: ~3,000 ejemplos (~$1.50)

### Entrenamiento Adversarial Cooperativo (self-play)
Los modelos se desafÃ­an mutuamente para mejorar:

```
Round 1: Detective genera frases con falacias ocultas
         â†’ Formalizador las formaliza
         â†’ Si la fÃ³rmula es "vÃ¡lida" pero hay falacia â†’ hard negative para Formalizador

Round 2: Formalizador genera fÃ³rmulas vÃ¡lidas
         â†’ Detective busca falacias
         â†’ Si dice "falacia" cuando no hay â†’ falso positivo para Detective

Round 3: Repeat â†’ ambos mejoran (estilo AlphaZero)
```

**Potencial de paper:**
> *"NanoLogic Stack: Formal and Informal Fallacy Detection through
> Adversarial Cooperation of Specialized Micro-Models"*

---

## ğŸ’° Control de costos

| Concepto | Costo | Estado |
|----------|-------|--------|
| Minado del dataset (2,334 ej.) | ~$1.00 | âœ… Completado |
| VerificaciÃ³n con API (2,210 ej.) | ~$0.50 | ğŸ”„ En progreso |
| Augmentation | $0.00 | â¬œ Pendiente (gratis, Python puro) |
| Preprocessing | $0.00 | â¬œ Pendiente (gratis, Python puro) |
| **Total Formalizador** | **~$1.50** | |
| Dataset Detective (futuro) | ~$1.50 | â¬œ Futuro |
| **Balance actual** | **$3.49** | Sobra de sobra |

---

## Herramientas

| Tool | Para quÃ© |
|------|----------|
| PyTorch | Construir el modelo |
| PyTorch Lightning | Orquestar el entrenamiento |
| HuggingFace Tokenizers | Entrenar el BPE tokenizer |
| Rich | Output bonito en terminal (barras de progreso, tablas) |
| TensorBoard | Visualizar mÃ©tricas de entrenamiento |
| Google Colab (T4) | GPU gratis para entrenar |
| DeepSeek API | Minado y verificaciÃ³n de datos |

---

## ğŸ“‹ Progreso

- [x] Minado del dataset (2,334 ejemplos raw)
- [x] Limpieza â€” `clean.py` (2,210 limpios, 94.7% retenciÃ³n)
- [x] VerificaciÃ³n â€” `verify.py` (2,118 verificados, 1,924 corregidos, 93 rechazados)
- [x] Augmentation â€” `augment.py` (6,761 ejemplos â€” 3x el original, gratis)
- [x] Preprocessing â€” `preprocess.py` (6,080 balanceados â†’ train/val/test)
- [x] Entrenar tokenizer BPE (vocab=8,000, guardado en models/tokenizer/)
- [ ] Construir modelo (Decoder-Only, ~15M params)
- [ ] Entrenar en Colab (T4)
- [ ] Evaluar y benchmark

### Resultado del pipeline completo:

```
dataset.json (2,334 raw)
    â†“ clean.py     â†’ 2,210 (94.7% retenciÃ³n)
    â†“ verify.py    â†’ 2,118 (1,924 corregidos por la API)
    â†“ augment.py   â†’ 6,761 (equivalencias lÃ³gicas + composiciones, gratis)
    â†“ preprocess.py â†’ 6,080 (balanceado por complejidad)
    âœ… LISTO â†’ data/processed/
```

### Split final:

| Split | Ejemplos | Archivo |
|-------|----------|---------|
| Train | 4,864 | `data/processed/train.jsonl` |
| Val | 617 | `data/processed/val.jsonl` |
| Test | 599 | `data/processed/test.jsonl` |

### DistribuciÃ³n del dataset final:

| Complejidad | % |
|-------------|---|
| Simple | 28% |
| Intermediate | 35% |
| Advanced | 37% |

| Conector | Presencia |
|----------|-----------|
| âˆ§ (AND) | 97.8% |
| Â¬ (NOT) | 83.0% |
| â†’ (IMPLICA) | 70.5% |
| âˆ¨ (OR) | 66.0% |
| â†” (BICOND) | 40.8% |

